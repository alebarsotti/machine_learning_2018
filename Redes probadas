nn_model = Sequential([
    Dense(1000, activation='tanh', input_shape=(len(INPUT_COLUMNS), )),
    Dropout(0.25),
    
    Dense(1000, activation='tanh'),
    Dropout(0.25),
    
    Dense(1000, activation='tanh'),
    Dropout(0.25),
    
    Dense(len(FRUITS), activation='softmax'),
])

    batch_size=128,


Train on 31688 samples, validate on 5328 samples
Epoch 1/5
31688/31688 [==============================] - 22s 685us/step - loss: 3.4238 - acc: 0.0925 - val_loss: 2.3832 - val_acc: 0.2481
Epoch 2/5
31688/31688 [==============================] - 22s 680us/step - loss: 2.7281 - acc: 0.1697 - val_loss: 2.2214 - val_acc: 0.3097
Epoch 3/5
31688/31688 [==============================] - 21s 677us/step - loss: 2.4531 - acc: 0.2239 - val_loss: 2.0101 - val_acc: 0.3390
Epoch 4/5
31688/31688 [==============================] - 22s 679us/step - loss: 2.2719 - acc: 0.2674 - val_loss: 1.7642 - val_acc: 0.4302
Epoch 5/5
31688/31688 [==============================] - 22s 682us/step - loss: 2.1545 - acc: 0.2924 - val_loss: 1.7819 - val_acc: 0.4022


*******************************************************************************************************************************

Misma red

batch_size=512

Train on 31688 samples, validate on 5328 samples
Epoch 1/5
31688/31688 [==============================] - 16s 496us/step - loss: 3.0224 - acc: 0.1858 - val_loss: 1.6662 - val_acc: 0.5118
Epoch 2/5
31688/31688 [==============================] - 15s 486us/step - loss: 1.2537 - acc: 0.5876 - val_loss: 0.7564 - val_acc: 0.7431
Epoch 3/5
31688/31688 [==============================] - 15s 482us/step - loss: 0.8474 - acc: 0.7120 - val_loss: 0.6062 - val_acc: 0.8029
Epoch 4/5
31688/31688 [==============================] - 17s 522us/step - loss: 0.5236 - acc: 0.8234 - val_loss: 0.4743 - val_acc: 0.8607
Epoch 5/5
31688/31688 [==============================] - 15s 482us/step - loss: 0.4763 - acc: 0.8343 - val_loss: 0.5339 - val_acc: 0.8373

*******************************************************************************************************************************

nn_model = Sequential([
    Dense(1000, activation='tanh', input_shape=(len(INPUT_COLUMNS), )),
    Dropout(0.25),
    
    Dense(500, activation='tanh'),
    Dropout(0.25),
    
    Dense(100, activation='tanh'),
    Dropout(0.25),
    
    Dense(len(FRUITS), activation='softmax'),
])

batch_size=512

Train on 31688 samples, validate on 5328 samples
Epoch 1/5
31688/31688 [==============================] - 15s 473us/step - loss: 3.8458 - acc: 0.0602 - val_loss: 2.9188 - val_acc: 0.1732
Epoch 2/5
31688/31688 [==============================] - 16s 503us/step - loss: 2.6706 - acc: 0.2109 - val_loss: 1.9952 - val_acc: 0.4448
Epoch 3/5
31688/31688 [==============================] - 17s 537us/step - loss: 1.9715 - acc: 0.3957 - val_loss: 1.4411 - val_acc: 0.6599
Epoch 4/5
31688/31688 [==============================] - 15s 478us/step - loss: 1.4814 - acc: 0.5519 - val_loss: 1.1403 - val_acc: 0.6890
Epoch 5/5
31688/31688 [==============================] - 15s 463us/step - loss: 1.1947 - acc: 0.6406 - val_loss: 0.8389 - val_acc: 0.7898


*******************************************************************************************************************************

nn_model = Sequential([
    Dense(1000, activation='tanh', input_shape=(len(INPUT_COLUMNS), )),
    Dropout(0.25),
    
    Dense(500, activation='relu'),
    Dropout(0.25),
    
    Dense(100, activation='sigmoid'),
    Dropout(0.25),
    
    Dense(len(FRUITS), activation='softmax'),
])

batch_size=512

Train on 31688 samples, validate on 5328 samples
Epoch 1/7
31688/31688 [==============================] - 15s 487us/step - loss: 3.9819 - acc: 0.0411 - val_loss: 3.4574 - val_acc: 0.0616
Epoch 2/7
31688/31688 [==============================] - 15s 465us/step - loss: 3.1309 - acc: 0.1250 - val_loss: 2.6675 - val_acc: 0.2843
Epoch 3/7
31688/31688 [==============================] - 15s 479us/step - loss: 2.5164 - acc: 0.2527 - val_loss: 2.1508 - val_acc: 0.4341
Epoch 4/7
31688/31688 [==============================] - 15s 478us/step - loss: 2.0749 - acc: 0.3719 - val_loss: 1.8613 - val_acc: 0.5197
Epoch 5/7
31688/31688 [==============================] - 15s 477us/step - loss: 1.7883 - acc: 0.4505 - val_loss: 1.5737 - val_acc: 0.5717
Epoch 6/7
31688/31688 [==============================] - 15s 472us/step - loss: 1.5560 - acc: 0.5284 - val_loss: 1.3647 - val_acc: 0.6503
Epoch 7/7
31688/31688 [==============================] - 16s 501us/step - loss: 1.4078 - acc: 0.5745 - val_loss: 1.1713 - val_acc: 0.6956

*******************************************************************************************************************************

nn_model = Sequential([
    Dense(500, activation='relu', input_shape=(len(INPUT_COLUMNS), )),
    Dropout(0.25),
    
    Dense(250, activation='relu'),
    Dropout(0.25),
    
    Dense(100, activation='relu'),
    Dropout(0.25),
    
    Dense(len(FRUITS), activation='softmax'),
])

nn_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy',],
)

epochs=6
batch_size=512

Train on 31688 samples, validate on 5328 samples
Epoch 1/7
31688/31688 [==============================] - 15s 488us/step - loss: 4.1997 - acc: 0.0377 - val_loss: 3.3959 - val_acc: 0.1038
Epoch 2/7
31688/31688 [==============================] - 14s 452us/step - loss: 2.8404 - acc: 0.2013 - val_loss: 1.6541 - val_acc: 0.5283
Epoch 3/7
31688/31688 [==============================] - 14s 457us/step - loss: 1.7557 - acc: 0.4462 - val_loss: 0.9310 - val_acc: 0.7665
Epoch 4/7
31688/31688 [==============================] - 14s 450us/step - loss: 1.2527 - acc: 0.5939 - val_loss: 0.6471 - val_acc: 0.8198
Epoch 5/7
31688/31688 [==============================] - 14s 449us/step - loss: 0.9764 - acc: 0.6776 - val_loss: 0.5396 - val_acc: 0.8463
Epoch 6/7
31688/31688 [==============================] - 14s 453us/step - loss: 0.8144 - acc: 0.7330 - val_loss: 0.4322 - val_acc: 0.8703
Epoch 7/7
31688/31688 [==============================] - 14s 455us/step - loss: 0.7142 - acc: 0.7674 - val_loss: 0.4555 - val_acc: 0.8727

*******************************************************************************************************************************

nn_model = Sequential([
    Dense(500, activation='relu', input_shape=(len(INPUT_COLUMNS), )),
    Dropout(0.25),
    
    Dense(250, activation='relu'),
    Dropout(0.25),
    
    Dense(100, activation='relu'),
    Dropout(0.25),
    
    Dense(len(FRUITS), activation='softmax'),
])

batch_size=256

Train on 31688 samples, validate on 5328 samples
Epoch 1/6
31688/31688 [==============================] - 19s 596us/step - loss: 3.5473 - acc: 0.1190 - val_loss: 1.6408 - val_acc: 0.5248
Epoch 2/6
31688/31688 [==============================] - 18s 574us/step - loss: 1.8086 - acc: 0.4323 - val_loss: 0.8903 - val_acc: 0.7333
Epoch 3/6
31688/31688 [==============================] - 18s 563us/step - loss: 1.2201 - acc: 0.5960 - val_loss: 0.6311 - val_acc: 0.8108
Epoch 4/6
31688/31688 [==============================] - 18s 573us/step - loss: 0.9501 - acc: 0.6834 - val_loss: 0.5174 - val_acc: 0.8412
Epoch 5/6
31688/31688 [==============================] - 17s 546us/step - loss: 0.8071 - acc: 0.7338 - val_loss: 0.4956 - val_acc: 0.8461
Epoch 6/6
31688/31688 [==============================] - 18s 557us/step - loss: 0.6737 - acc: 0.7781 - val_loss: 0.4574 - val_acc: 0.8423

